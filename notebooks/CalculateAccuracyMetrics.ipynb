{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pathlib in ./.venv/lib/python3.12/site-packages (1.0.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "metrics_output_dir = Path('./res/outputs/adjudication_metrics')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OCR Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import json\n",
    "ocr_output_dir = Path('./res/outputs/ocr')\n",
    "\n",
    "#we construct the global list of claims that this process will use\n",
    "def did_complete_ocr(claim) -> bool:\n",
    "   #ocr is complete if the claim[\"documents\"] list is not-empty. \n",
    "   return len(claim[\"documents\"]) > 0\n",
    "\n",
    "#read through every json file in the ocr output directory and build a dataframe of the policy id, claim id, and number of documents in the claim\n",
    "claims_batch_df = pd.DataFrame()\n",
    "for file in ocr_output_dir.glob(\"*.json\"):\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "        number_of_documents_in_claim = len(data['documents'])\n",
    "        did_claim_complete_ocr = did_complete_ocr(data)\n",
    "        new_row = pd.DataFrame({'policy_id': [data['policy_id']], 'claim_id': [data['claim_id']], 'number_of_documents_in_claim': [number_of_documents_in_claim], 'did_claim_complete_ocr': [did_claim_complete_ocr]})\n",
    "\n",
    "        claims_batch_df = pd.concat([claims_batch_df, new_row], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data types for join keys in claims_batch_df:\n",
      "policy_id    object\n",
      "claim_id     object\n",
      "dtype: object\n",
      "\n",
      "Unique values in 'gt_decision' (original from CSV after replace): ['PAY' 'DENY']\n",
      "Number of DENY records in gt_df before drop_duplicates: 62\n",
      "Number of DENY records in gt_df after drop_duplicates: 53\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import logging\n",
    "\n",
    "# --- Assuming claims_batch_df is already loaded from the previous cell ---\n",
    "# Ensure join key columns in claims_batch_df are strings, just to be safe\n",
    "if 'policy_id' in claims_batch_df.columns:\n",
    "    claims_batch_df['policy_id'] = claims_batch_df['policy_id'].astype(str)\n",
    "if 'claim_id' in claims_batch_df.columns:\n",
    "    claims_batch_df['claim_id'] = claims_batch_df['claim_id'].astype(str)\n",
    "\n",
    "print(\"Data types for join keys in claims_batch_df:\")\n",
    "if 'policy_id' in claims_batch_df.columns and 'claim_id' in claims_batch_df.columns:\n",
    "    print(claims_batch_df[['policy_id', 'claim_id']].dtypes)\n",
    "else:\n",
    "    print(\"policy_id or claim_id not in claims_batch_df at this point.\")\n",
    "\n",
    "\n",
    "num_claims_in_batch = len(claims_batch_df)\n",
    "num_claims_in_batch_that_did_complete_ocr = len(claims_batch_df[claims_batch_df['did_claim_complete_ocr'] == True])\n",
    "num_claims_in_batch_that_did_not_complete_ocr = len(claims_batch_df[claims_batch_df['did_claim_complete_ocr'] == False])\n",
    "\n",
    "adjudication_output_dir = Path('./res/outputs/adjudication')\n",
    "ground_truth_file = Path('./res/outputs/ground_truth_claims_decisions/policy_claims_decisions.csv')\n",
    "\n",
    "#Load the ground truth data frame.\n",
    "gt_df = pd.read_csv(ground_truth_file, dtype={'policy_number': str, 'claim_number': str})\n",
    "gt_df['decision'] = gt_df['decision'].astype(str).str.upper()\n",
    "gt_df['decision'] = gt_df['decision'].replace({'PAID': 'PAY', 'DENIED': 'DENY'})\n",
    "gt_df = gt_df.rename(columns={'policy_number': 'policy_id', 'claim_number': 'claim_id', 'decision': 'gt_decision', 'status': 'gt_status', 'status_reason': 'gt_status_reason','in_data_set': 'gt_in_data_set'})\n",
    "\n",
    "print(\"\\nUnique values in 'gt_decision' (original from CSV after replace):\", gt_df['gt_decision'].unique())\n",
    "print(f\"Number of DENY records in gt_df before drop_duplicates: {len(gt_df[gt_df['gt_decision'] == 'DENY'])}\")\n",
    "\n",
    "# Drop duplicates from gt_df based on the join keys before merging\n",
    "gt_df_deduplicated = gt_df.drop_duplicates(subset=['policy_id', 'claim_id'], keep='first')\n",
    "\n",
    "print(f\"Number of DENY records in gt_df after drop_duplicates: {len(gt_df_deduplicated[gt_df_deduplicated['gt_decision'] == 'DENY'])}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of unique (policy_id, claim_id) keys in claims_batch_df: 94\n",
      "Number of unique (policy_id, claim_id) keys for DENY in gt_df_deduplicated: 53\n",
      "Number of DENY keys from gt_df that are also found in claims_batch_df: 33\n",
      "Sample of matching DENY keys found in both DataFrames:\n",
      "[('4151524576', '202410100052'), ('4151516802', '202406170099'), ('4151582003', '202502060010'), ('4151525980', '202411010051'), ('4151484579', '202503280007')]\n",
      "\n",
      "Number of rows in merged claims_batch_df with missing gt_decision: 0\n",
      "\n",
      "Unique values in 'gt_decision' in the merged claims_batch_df:\n",
      "['PAY' 'DENY']\n",
      "Value counts for 'gt_decision' in the merged claims_batch_df:\n",
      "gt_decision\n",
      "PAY     61\n",
      "DENY    33\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# --- New Diagnostic Section ---\n",
    "# Get the set of (policy_id, claim_id) tuples from claims_batch_df\n",
    "claims_batch_keys = set(zip(claims_batch_df['policy_id'], claims_batch_df['claim_id']))\n",
    "\n",
    "# Get the (policy_id, claim_id) pairs that are \"DENY\" in the deduplicated gt_df\n",
    "gt_deny_keys_df = gt_df_deduplicated[gt_df_deduplicated['gt_decision'] == 'DENY'][['policy_id', 'claim_id']]\n",
    "gt_deny_keys = set(zip(gt_deny_keys_df['policy_id'], gt_deny_keys_df['claim_id']))\n",
    "\n",
    "print(f\"\\nNumber of unique (policy_id, claim_id) keys in claims_batch_df: {len(claims_batch_keys)}\")\n",
    "print(f\"Number of unique (policy_id, claim_id) keys for DENY in gt_df_deduplicated: {len(gt_deny_keys)}\")\n",
    "\n",
    "# Find which \"DENY\" keys from gt_df are present in claims_batch_df\n",
    "matching_deny_keys = claims_batch_keys.intersection(gt_deny_keys)\n",
    "print(f\"Number of DENY keys from gt_df that are also found in claims_batch_df: {len(matching_deny_keys)}\")\n",
    "\n",
    "if len(matching_deny_keys) > 0:\n",
    "    print(\"Sample of matching DENY keys found in both DataFrames:\")\n",
    "    print(list(matching_deny_keys)[:5])\n",
    "else:\n",
    "    print(\"No DENY keys from gt_df were found in claims_batch_df.\")\n",
    "    if gt_deny_keys: # Corrected line: check if the set is not empty\n",
    "        print(\"Sample of DENY keys from gt_df (that were not found in claims_batch_df):\")\n",
    "        print(list(gt_deny_keys)[:5])\n",
    "# --- End New Diagnostic Section ---\n",
    "\n",
    "\n",
    "#Now we want to perform a left outer join of the claims_batch_df with the gt_df on the policy_id and claim_id fields.\n",
    "claims_batch_df = pd.merge(\n",
    "    claims_batch_df,\n",
    "    gt_df_deduplicated[['policy_id', 'claim_id', 'gt_decision', 'gt_status', 'gt_status_reason']], # use deduplicated gt_df\n",
    "    on=['policy_id', 'claim_id'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "missing_gt_decision_df = claims_batch_df[claims_batch_df['gt_decision'].isnull()]\n",
    "print(f\"\\nNumber of rows in merged claims_batch_df with missing gt_decision: {len(missing_gt_decision_df)}\")\n",
    "\n",
    "print(\"\\nUnique values in 'gt_decision' in the merged claims_batch_df:\")\n",
    "print(claims_batch_df['gt_decision'].unique())\n",
    "print(\"Value counts for 'gt_decision' in the merged claims_batch_df:\")\n",
    "print(claims_batch_df['gt_decision'].value_counts(dropna=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's not filter out any rows from the claims_batch_df that did NOT complete OCR.\n",
    "\n",
    "claims_batch_df_with_ocr_results = claims_batch_df[claims_batch_df['did_claim_complete_ocr'] == True]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Go through each JSON file in the adjudication_output_dir and extract the claim_id and policy_id from \n",
    "#json data. Then use the claim and policy ids to add the json data to the claims_batch_df\n",
    "adjudication_data_df = pd.DataFrame()\n",
    "for file in adjudication_output_dir.glob(\"*.json\"):\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "        claim_id = data['claim_id']\n",
    "        policy_id = data['policy_id']\n",
    "        adjudication_decision = (data['decision']['status']).upper()\n",
    "       \n",
    "        new_row = pd.DataFrame({'policy_id': [policy_id], 'claim_id': [claim_id], 'adjudication_decision': [adjudication_decision], 'adjudication_data': [data]})\n",
    "        adjudication_data_df = pd.concat([adjudication_data_df, new_row], ignore_index=True)\n",
    "\n",
    "#Now we have a claims_batch_df with the adjudication data added to it.\n",
    "#Take the claims_batch_df_with_ocr_results and left outer join it with the adjudication_data_df on the policy_id and claim_id fields.\n",
    "claims_batch_df_with_ocr_results = pd.merge(claims_batch_df_with_ocr_results, adjudication_data_df, on=['policy_id', 'claim_id'], how='left')\n",
    "\n",
    "num_claims_marked_refer = len(claims_batch_df_with_ocr_results[claims_batch_df_with_ocr_results['adjudication_decision'] == 'REFER'])\n",
    "num_claims_marked_pay = len(claims_batch_df_with_ocr_results[claims_batch_df_with_ocr_results['adjudication_decision'] == 'PAY'])\n",
    "num_claims_marked_deny = len(claims_batch_df_with_ocr_results[claims_batch_df_with_ocr_results['adjudication_decision'] == 'DENY'])\n",
    "\n",
    "#We need to filter all the rows where the adjudication_devision is \"REFER\"\n",
    "claims_batch_df_with_ocr_results = claims_batch_df_with_ocr_results[claims_batch_df_with_ocr_results['adjudication_decision'] != 'REFER']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Initial Data Overview for 'gt_decision' and 'adjudication_decision' ---\n",
      "\n",
      "Value counts for 'gt_decision':\n",
      "gt_decision\n",
      "PAY     57\n",
      "DENY    27\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Value counts for 'adjudication_decision':\n",
      "adjudication_decision\n",
      "PAY     76\n",
      "DENY     8\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- Preparing data for metrics calculation ---\n",
      "Removed 0 rows due to missing 'gt_decision'.\n",
      "No NaN values found in 'adjudication_decision' for rows with valid 'gt_decision'.\n",
      "\n",
      "--- Calculating Metrics ---\n",
      "Unique labels considered for metrics: ['DENY', 'PAY']\n",
      "\n",
      "Metrics summary saved to JSON: res/outputs/adjudication_metrics/adjudication_summary_statistics_20250518_163523.json\n",
      "Metrics summary saved to Markdown: res/outputs/adjudication_metrics/adjudication_summary_statistics_20250518_163523.md\n",
      "\n",
      "Accuracy: 0.7738\n",
      "\n",
      "Precision per class:\n",
      "  DENY: 1.0000\n",
      "  PAY: 0.7500\n",
      "\n",
      "Recall per class:\n",
      "  DENY: 0.2963\n",
      "  PAY: 1.0000\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        DENY       1.00      0.30      0.46        27\n",
      "         PAY       0.75      1.00      0.86        57\n",
      "\n",
      "    accuracy                           0.77        84\n",
      "   macro avg       0.88      0.65      0.66        84\n",
      "weighted avg       0.83      0.77      0.73        84\n",
      "\n",
      "\n",
      "--- Interpretation Notes (also in Markdown file) ---\n",
      "* Metrics are calculated on 84 data points after removing rows with missing 'gt_decision'.\n",
      "* 'Support' in the classification report refers to the number of actual instances of each class in 'gt_decision'.\n",
      "* If a class has 0 support (no true instances), its recall and F1-score will be 0.\n",
      "* If a class was never predicted, its precision might be 0 (depending on true instances).\n"
     ]
    }
   ],
   "source": [
    "# In[9]\n",
    "# Generate summary statistics for the claims_batch_df_with_ocr_results dataframe.\n",
    "# Treat the column adjudication_decision to be the predicted class and the column gt_decision to be the ground truth class.\n",
    "\n",
    "# You might need to install scikit-learn if you haven't already\n",
    "# !pip install scikit-learn\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report\n",
    "import json\n",
    "from datetime import datetime\n",
    "from pathlib import Path # Ensure Path is imported\n",
    "\n",
    "true_column = 'gt_decision'\n",
    "pred_column = 'adjudication_decision'\n",
    "\n",
    "print(f\"--- Initial Data Overview for '{true_column}' and '{pred_column}' ---\")\n",
    "if true_column in claims_batch_df_with_ocr_results:\n",
    "    print(f\"\\nValue counts for '{true_column}':\")\n",
    "    print(claims_batch_df_with_ocr_results[true_column].value_counts(dropna=False))\n",
    "else:\n",
    "    print(f\"\\nError: True column '{true_column}' not found in the DataFrame.\")\n",
    "    # exit() # Or handle error appropriately\n",
    "\n",
    "if pred_column in claims_batch_df_with_ocr_results:\n",
    "    print(f\"\\nValue counts for '{pred_column}':\")\n",
    "    print(claims_batch_df_with_ocr_results[pred_column].value_counts(dropna=False))\n",
    "else:\n",
    "    print(f\"\\nError: Predicted column '{pred_column}' not found in the DataFrame.\")\n",
    "    # exit() # Or handle error appropriately\n",
    "\n",
    "print(\"\\n--- Preparing data for metrics calculation ---\")\n",
    "# Create a copy for metrics calculation\n",
    "metrics_df = claims_batch_df_with_ocr_results[[true_column, pred_column]].copy()\n",
    "\n",
    "# Drop rows where ground truth is missing, as they cannot be used for evaluation\n",
    "initial_rows = len(metrics_df)\n",
    "metrics_df.dropna(subset=[true_column], inplace=True)\n",
    "rows_after_gt_dropna = len(metrics_df)\n",
    "removed_rows_count = initial_rows - rows_after_gt_dropna\n",
    "print(f\"Removed {removed_rows_count} rows due to missing '{true_column}'.\")\n",
    "\n",
    "if metrics_df.empty:\n",
    "    print(\"No data remaining after removing missing ground truth values. Cannot calculate metrics.\")\n",
    "else:\n",
    "    y_true = metrics_df[true_column]\n",
    "    y_pred = metrics_df[pred_column].copy() # Use .copy() to avoid SettingWithCopyWarning on fillna\n",
    "\n",
    "    # Handle potential NaNs in the prediction column for rows where ground truth is known.\n",
    "    y_pred_original_nans = y_pred.isnull().sum()\n",
    "    if y_pred_original_nans > 0:\n",
    "        print(f\"Found {y_pred_original_nans} NaN values in '{pred_column}' for rows with valid '{true_column}'. Replacing with 'NO_PREDICTION'.\")\n",
    "        y_pred.fillna(\"NO_PREDICTION\", inplace=True)\n",
    "    else:\n",
    "        print(f\"No NaN values found in '{pred_column}' for rows with valid '{true_column}'.\")\n",
    "\n",
    "    print(\"\\n--- Calculating Metrics ---\")\n",
    "\n",
    "    # Determine all unique labels present in true and predicted values\n",
    "    labels = sorted(list(set(y_true) | set(y_pred)))\n",
    "    print(f\"Unique labels considered for metrics: {labels}\")\n",
    "    \n",
    "    if not labels:\n",
    "        print(\"No labels found to calculate metrics.\")\n",
    "    else:\n",
    "        # Accuracy\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        \n",
    "        # Precision per class\n",
    "        precision_per_class_values = precision_score(y_true, y_pred, labels=labels, average=None, zero_division=0)\n",
    "        precision_per_class_dict = {label: score for label, score in zip(labels, precision_per_class_values)}\n",
    "\n",
    "        # Recall per class\n",
    "        recall_per_class_values = recall_score(y_true, y_pred, labels=labels, average=None, zero_division=0)\n",
    "        recall_per_class_dict = {label: score for label, score in zip(labels, recall_per_class_values)}\n",
    "        \n",
    "        # Classification Report (provides precision, recall, F1-score, and support)\n",
    "        report_str = classification_report(y_true, y_pred, labels=labels, zero_division=0)\n",
    "        \n",
    "        # Calculate actual counts from y_true (for data used in metrics)\n",
    "        actual_pay_in_metrics_data = (y_true == 'PAY').sum()\n",
    "        actual_deny_in_metrics_data = (y_true == 'DENY').sum()\n",
    "\n",
    "        # --- Prepare for file output ---\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        base_filename = f\"adjudication_summary_statistics_{timestamp}\"\n",
    "        \n",
    "        if isinstance(metrics_output_dir, str):\n",
    "            metrics_output_dir = Path(metrics_output_dir)\n",
    "            \n",
    "        json_filename = metrics_output_dir / f\"{base_filename}.json\"\n",
    "        md_filename = metrics_output_dir / f\"{base_filename}.md\"\n",
    "\n",
    "        # --- JSON Output ---\n",
    "        metrics_summary_json = {\n",
    "            \"generation_timestamp\": datetime.now().isoformat(),\n",
    "            \"data_pipeline_summary\": {\n",
    "                \"total_claims_in_initial_batch\": num_claims_in_batch,\n",
    "                \"claims_completed_ocr\": num_claims_in_batch_that_did_complete_ocr,\n",
    "                \"claims_predicted_pay_by_adjudication_before_refer_filter\": num_claims_marked_pay, \n",
    "                \"claims_predicted_deny_by_adjudication_before_refer_filter\": num_claims_marked_deny,\n",
    "                \"claims_predicted_refer_by_adjudication\": num_claims_marked_refer \n",
    "            },\n",
    "            \"metrics_calculation_summary\": {\n",
    "                \"true_column\": true_column,\n",
    "                \"predicted_column\": pred_column,\n",
    "                \"data_points_used_for_metrics\": len(y_true),\n",
    "                \"actual_pay_in_metrics_data\": int(actual_pay_in_metrics_data),\n",
    "                \"actual_deny_in_metrics_data\": int(actual_deny_in_metrics_data),\n",
    "                \"rows_removed_due_to_missing_ground_truth\": removed_rows_count,\n",
    "                \"predicted_nans_replaced_with_NO_PREDICTION\": int(y_pred_original_nans),\n",
    "                \"unique_labels_in_metrics\": labels\n",
    "            },\n",
    "            \"overall_accuracy\": accuracy,\n",
    "            \"precision_per_class\": precision_per_class_dict,\n",
    "            \"recall_per_class\": recall_per_class_dict,\n",
    "            \"classification_report_text\": report_str\n",
    "        }\n",
    "        \n",
    "        json_filename.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        with open(json_filename, 'w') as f_json:\n",
    "            json.dump(metrics_summary_json, f_json, indent=4)\n",
    "        print(f\"\\nMetrics summary saved to JSON: {json_filename}\")\n",
    "\n",
    "        # --- Markdown Output ---\n",
    "        md_content = []\n",
    "        md_content.append(f\"# Adjudication Summary Statistics\\n\")\n",
    "        md_content.append(f\"**Generated at:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        \n",
    "        md_content.append(f\"## Data Pipeline & Adjudication Counts\")\n",
    "        md_content.append(f\"*   Total claims in initial batch (from OCR output): **{num_claims_in_batch}**\")\n",
    "        md_content.append(f\"*   Claims that completed OCR: **{num_claims_in_batch_that_did_complete_ocr}**\")\n",
    "        md_content.append(f\"*   Claims predicted as PAY by adjudication (before 'REFER' filter for metrics): **{num_claims_marked_pay}**\")\n",
    "        md_content.append(f\"*   Claims predicted as DENY by adjudication (before 'REFER' filter for metrics): **{num_claims_marked_deny}**\")\n",
    "        md_content.append(f\"*   Claims predicted as REFER by adjudication: **{num_claims_marked_refer}**\\n\")\n",
    "\n",
    "        md_content.append(f\"## Metrics Calculation Overview\")\n",
    "        md_content.append(f\"*   Metrics calculated on **{len(y_true)}** data points (from OCR-completed, non-referred, and having ground truth).\")\n",
    "        md_content.append(f\"    *   Actual PAY decisions in this metrics set: **{actual_pay_in_metrics_data}**\")\n",
    "        md_content.append(f\"    *   Actual DENY decisions in this metrics set: **{actual_deny_in_metrics_data}**\")\n",
    "        md_content.append(f\"*   {removed_rows_count} rows were removed due to missing '{true_column}'.\")\n",
    "        if y_pred_original_nans > 0:\n",
    "            md_content.append(f\"*   **{y_pred_original_nans}** NaN values in '{pred_column}' (for rows with valid '{true_column}') were treated as 'NO_PREDICTION'.\")\n",
    "        else:\n",
    "            md_content.append(f\"*   No NaN values in '{pred_column}' (for rows with valid '{true_column}') needed replacement.\")\n",
    "        md_content.append(f\"*   Unique labels considered for metrics: `{labels}`\\n\")\n",
    "\n",
    "        md_content.append(f\"## Overall Accuracy\")\n",
    "        md_content.append(f\"Accuracy: {accuracy:.4f}\\n\")\n",
    "\n",
    "        md_content.append(f\"## Precision per Class\")\n",
    "        for label_item, score_item in precision_per_class_dict.items(): # Renamed to avoid conflict\n",
    "            md_content.append(f\"*   {label_item}: {score_item:.4f}\")\n",
    "        md_content.append(\"\\n\")\n",
    "\n",
    "        md_content.append(f\"## Recall per Class\")\n",
    "        for label_item, score_item in recall_per_class_dict.items(): # Renamed to avoid conflict\n",
    "            md_content.append(f\"*   {label_item}: {score_item:.4f}\")\n",
    "        md_content.append(\"\\n\")\n",
    "        \n",
    "        md_content.append(f\"## Classification Report\")\n",
    "        md_content.append(f\"```\")\n",
    "        md_content.append(report_str)\n",
    "        md_content.append(f\"```\\n\")\n",
    "\n",
    "        # --- New Section for Detailed Claims Data Table ---\n",
    "        md_content.append(f\"## Detailed Claims Data (Metrics Set)\\n\")\n",
    "        md_content.append(f\"The following table shows the specific claims that were included in the metrics calculation (after filtering for OCR completion, non-'REFER' adjudication, and presence of a ground truth decision).\\n\")\n",
    "\n",
    "        columns_to_dump = [\n",
    "            'policy_id', \n",
    "            'claim_id', \n",
    "            'number_of_documents_in_claim', \n",
    "            'did_claim_complete_ocr', \n",
    "            'gt_decision', \n",
    "            'gt_status_reason',\n",
    "            'adjudication_decision', \n",
    "            'adjudication_data'\n",
    "        ]\n",
    "        \n",
    "        # Create the DataFrame for the table based on the indices of y_true\n",
    "        # This ensures it only contains rows used in the metrics.\n",
    "        df_for_markdown_table_source = claims_batch_df_with_ocr_results.loc[y_true.index].copy()\n",
    "        \n",
    "        # Override 'adjudication_decision' with y_pred (which has NaNs filled)\n",
    "        df_for_markdown_table_source['adjudication_decision'] = y_pred\n",
    "\n",
    "        # Ensure all columns are present, add as NA if missing, and select in specified order\n",
    "        temp_table_data = {}\n",
    "        for col_name in columns_to_dump:\n",
    "            if col_name in df_for_markdown_table_source.columns:\n",
    "                temp_table_data[col_name] = df_for_markdown_table_source[col_name]\n",
    "            else:\n",
    "                # If a column is genuinely missing and expected, this will add it with NAs.\n",
    "                # For 'adjudication_decision', it's already handled by y_pred assignment.\n",
    "                print(f\"Warning: Column '{col_name}' not found for detailed table, will be empty.\")\n",
    "                temp_table_data[col_name] = pd.Series([pd.NA] * len(df_for_markdown_table_source), index=df_for_markdown_table_source.index)\n",
    "        \n",
    "        df_for_markdown_table = pd.DataFrame(temp_table_data, columns=columns_to_dump)\n",
    "\n",
    "\n",
    "        def format_json_cell(data):\n",
    "            if pd.isna(data):\n",
    "                return \"\"\n",
    "            try:\n",
    "                # Compact JSON string\n",
    "                json_str = json.dumps(data, separators=(',', ':'))\n",
    "                # Escape backticks within the JSON string itself, then wrap with backticks\n",
    "                json_str_escaped = json_str.replace('`', '\\\\`')\n",
    "                return f\"`{json_str_escaped}`\"\n",
    "            except TypeError:\n",
    "                return \"`Error: Not JSON serializable`\"\n",
    "            except Exception as e: # Catch any other unexpected error during formatting\n",
    "                return f\"`Error formatting JSON: {str(e)}`\"\n",
    "\n",
    "        if 'adjudication_data' in df_for_markdown_table.columns:\n",
    "            df_for_markdown_table['adjudication_data'] = df_for_markdown_table['adjudication_data'].apply(format_json_cell)\n",
    "\n",
    "        # Fill remaining NaN in other columns with empty string for cleaner Markdown table\n",
    "        df_for_markdown_table = df_for_markdown_table.fillna('') \n",
    "\n",
    "        if not df_for_markdown_table.empty:\n",
    "            markdown_table_string = df_for_markdown_table.to_markdown(index=False)\n",
    "            md_content.append(markdown_table_string)\n",
    "        else:\n",
    "            md_content.append(\"No data available to display in the detailed claims table (metrics dataset was empty or table generation failed).\")\n",
    "        md_content.append(\"\\n\")\n",
    "        # --- End of New Section ---\n",
    "        \n",
    "        md_content.append(f\"## Interpretation Notes\")\n",
    "        md_content.append(f\"*   Metrics are calculated on {len(y_true)} data points after removing rows with missing '{true_column}'.\")\n",
    "        if y_pred_original_nans > 0:\n",
    "            md_content.append(f\"*   If '{pred_column}' had NaN values, they were treated as a 'NO_PREDICTION' category.\")\n",
    "        md_content.append(f\"*   'Support' in the classification report refers to the number of actual instances of each class in '{true_column}'.\")\n",
    "        md_content.append(f\"*   If a class has 0 support (no true instances), its recall and F1-score will be 0.\")\n",
    "        md_content.append(f\"*   If a class was never predicted, its precision might be 0 (depending on true instances).\")\n",
    "        if 'DENY' not in y_true.unique() and 'DENY' in labels: # Check y_true.unique() instead of labels for this warning\n",
    "             md_content.append(f\"\\n**WARNING:** The ground truth data used for these metrics ('{true_column}') does not appear to contain 'DENY' labels among the {len(y_true)} records used.\")\n",
    "             md_content.append(f\"This means recall for 'DENY' will be 0, and precision for 'DENY' will also be 0 if 'DENY' was predicted for any non-DENY true case.\")\n",
    "        \n",
    "        with open(md_filename, 'w', encoding='utf-8') as f_md: # Added encoding\n",
    "            f_md.write(\"\\n\".join(md_content))\n",
    "        print(f\"Metrics summary saved to Markdown: {md_filename}\")\n",
    "\n",
    "        # Display the original print outputs as well for immediate notebook feedback\n",
    "        print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
    "        print(\"\\nPrecision per class:\")\n",
    "        for label_item, score_item in precision_per_class_dict.items(): # Renamed to avoid conflict\n",
    "            print(f\"  {label_item}: {score_item:.4f}\")\n",
    "        print(\"\\nRecall per class:\")\n",
    "        for label_item, score_item in recall_per_class_dict.items(): # Renamed to avoid conflict\n",
    "            print(f\"  {label_item}: {score_item:.4f}\")\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(report_str)\n",
    "        \n",
    "        print(\"\\n--- Interpretation Notes (also in Markdown file) ---\")\n",
    "        print(f\"* Metrics are calculated on {len(y_true)} data points after removing rows with missing '{true_column}'.\")\n",
    "        if y_pred_original_nans > 0:\n",
    "            print(f\"* If '{pred_column}' had NaN values, they were treated as a 'NO_PREDICTION' category.\")\n",
    "        print(f\"* 'Support' in the classification report refers to the number of actual instances of each class in '{true_column}'.\")\n",
    "        print(\"* If a class has 0 support (no true instances), its recall and F1-score will be 0.\")\n",
    "        print(\"* If a class was never predicted, its precision might be 0 (depending on true instances).\")\n",
    "        if 'DENY' not in y_true.unique() and 'DENY' in labels: # Check y_true.unique()\n",
    "             print(f\"\\nWARNING: The ground truth data used for these metrics ('{true_column}') does not appear to contain 'DENY' labels among the {len(y_true)} records used.\")\n",
    "             print(\"This means recall for 'DENY' will be 0, and precision for 'DENY' will also be 0 if 'DENY' was predicted for any non-DENY true case.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
